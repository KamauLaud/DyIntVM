{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code  \n",
    "---\n",
    "This code is taken from: https://github.com/rafikg/CEAL\n",
    "## Import all the required stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imported from run_ceal/ceal_learning_algorithm.py\n",
    "from model import AlexNet\n",
    "from utils import Normalize, RandomCrop, SquarifyImage, \\\n",
    "    ToTensor, GameImageDataset\n",
    "from utils import get_uncertain_samples, get_high_confidence_samples, \\\n",
    "    update_threshold\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "\n",
    "# others\n",
    "from sklearn.metrics import precision_recall_fscore_support as prfs\n",
    "\n",
    "logging.basicConfig(format=\"%(levelname)s:%(name)s: %(message)s\",\n",
    "                    level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CEAL Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ceal_learning_algorithm(du: DataLoader,\n",
    "                            dl: DataLoader,\n",
    "                            dtest: DataLoader,\n",
    "                            k: int = 20,\n",
    "                            delta_0: float = 0.005,\n",
    "                            dr: float = 0.00033,\n",
    "                            t: int = 1,\n",
    "                            epochs: int = 10,\n",
    "                            criteria: str = 'cl',\n",
    "                            max_iter: int = 15):\n",
    "    \"\"\"\n",
    "    Algorithm1 : Learning algorithm of CEAL.\n",
    "    For simplicity, I used the same notation in the paper.\n",
    "    Parameters\n",
    "    ----------\n",
    "    du: DataLoader\n",
    "        Unlabeled samples\n",
    "    dl : DataLoader\n",
    "        labeled samples\n",
    "    dtest : DataLoader\n",
    "        test data\n",
    "    k: int, (default = 1000)\n",
    "        uncertain samples selection\n",
    "    delta_0: float\n",
    "        hight confidence samples selection threshold\n",
    "    dr: float\n",
    "        threshold decay\n",
    "    t: int\n",
    "        fine-tuning interval\n",
    "    epochs: int\n",
    "    criteria: str\n",
    "    max_iter: int\n",
    "        maximum iteration number.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    \"\"\"\n",
    "    logger.info('Initial configuration: len(du): {}, len(dl): {} '.format(\n",
    "        len(du.sampler.indices),\n",
    "        len(dl.sampler.indices)))\n",
    "\n",
    "    # Create the model\n",
    "    model = AlexNet(n_classes=4, device=device)\n",
    "\n",
    "    # Initialize the model\n",
    "    logger.info('Intialize training the model on `dl` and test on `dtest`')\n",
    "\n",
    "    model.train(epochs=epochs, train_loader=dl, valid_loader=None)\n",
    "\n",
    "    # Evaluate model on dtest\n",
    "    p, r, f = model.evaluate(test_loader=dtest)\n",
    "\n",
    "    print('====> Initial precision: {} '.format(sum(p)/4))\n",
    "    print('====> Initial recall: {} '.format(sum(r)/4))\n",
    "    print('====> Initial f-score: {} '.format(sum(f)/4))\n",
    "\n",
    "    for iteration in range(max_iter):\n",
    "\n",
    "        logger.info('Iteration: {}: run prediction on unlabeled data '\n",
    "                    '`du` '.format(iteration))\n",
    "\n",
    "        pred_prob = model.predict(test_loader=du)\n",
    "\n",
    "        # get k uncertain samples\n",
    "        uncert_samp_idx, _ = get_uncertain_samples(pred_prob=pred_prob, k=k,\n",
    "                                                   criteria=criteria)\n",
    "\n",
    "        # get original indices\n",
    "        uncert_samp_idx = [du.sampler.indices[idx] for idx in uncert_samp_idx]\n",
    "\n",
    "        # add the uncertain samples selected from `du` to the labeled samples\n",
    "        #  set `dl`\n",
    "        dl.sampler.indices.extend(uncert_samp_idx)\n",
    "\n",
    "        logger.info(\n",
    "            'Update size of `dl`  and `du` by adding uncertain {} samples'\n",
    "            ' in `dl`'\n",
    "            ' len(dl): {}, len(du) {}'.\n",
    "            format(len(uncert_samp_idx), len(dl.sampler.indices),\n",
    "                   len(du.sampler.indices)))\n",
    "\n",
    "        # get high confidence samples `dh`\n",
    "        hcs_idx, hcs_labels = get_high_confidence_samples(pred_prob=pred_prob,\n",
    "                                                          delta=delta_0)\n",
    "        # get the original indices\n",
    "        hcs_idx = [du.sampler.indices[idx] for idx in hcs_idx]\n",
    "\n",
    "        # remove the samples that already selected as uncertain samples.\n",
    "        hcs_idx = [x for x in hcs_idx if\n",
    "                   x not in list(set(uncert_samp_idx) & set(hcs_idx))]\n",
    "\n",
    "        # add high confidence samples to the labeled set 'dl'\n",
    "\n",
    "        # (1) update the indices\n",
    "        dl.sampler.indices.extend(hcs_idx)\n",
    "        # (2) update the original labels with the pseudo labels.\n",
    "        for idx in range(len(hcs_idx)):\n",
    "            x = [0, 0, 0, 0]\n",
    "            x[hcs_labels[idx]] = 1    \n",
    "            dl.dataset.labels[hcs_idx[idx]] = x\n",
    "        logger.info(\n",
    "            'Update size of `dl`  and `du` by adding {} hcs samples in `dl`'\n",
    "            ' len(dl): {}, len(du) {}'.\n",
    "            format(len(hcs_idx), len(dl.sampler.indices),\n",
    "                   len(du.sampler.indices)))\n",
    "\n",
    "        if iteration % t == 0:\n",
    "            logger.info('Iteration: {} fine-tune the model on dh U dl'.\n",
    "                        format(iteration))\n",
    "            model.train(epochs=epochs, train_loader=dl)\n",
    "\n",
    "            # update delta_0\n",
    "            delta_0 = update_threshold(delta=delta_0, dr=dr, t=iteration)\n",
    "\n",
    "        # remove the uncertain samples from the original `du`\n",
    "        logger.info('remove {} uncertain samples from du'.\n",
    "                    format(len(uncert_samp_idx)))\n",
    "        for val in uncert_samp_idx:\n",
    "            du.sampler.indices.remove(val)\n",
    "\n",
    "        p, r, f = model.evaluate(test_loader=dtest)\n",
    "                \n",
    "        print(\n",
    "            \"Iteration: {}, len(dl): {}, len(du): {},\"\n",
    "            \" len(dh) {}\".format(\n",
    "                iteration, len(dl.sampler.indices),\n",
    "                len(du.sampler.indices), len(hcs_idx)))\n",
    "        \n",
    "        print(\"Precision:\",sum(p)/4)\n",
    "        print(\"Recall:\",sum(r)/4)\n",
    "        print(\"F1:\",sum(f)/4)\n",
    "        \n",
    "    print(\"Saving model...\")\n",
    "    torch.save(model, 'saved_model/alexnet.pth')\n",
    "    print(\"Model saved as: saved_model/alexnet.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__: Initial configuration: len(du): 314, len(dl): 34 \n",
      "INFO:model.alexnet: The code is running on cuda:1 \n",
      "INFO:__main__: Intialize training the model on `dl` and test on `dtest`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/34 (0%)]\tLoss: 0.714026\n",
      "====> Epoch: 0 Average loss: 0.0707\n",
      "Train Epoch: 1 [0/34 (0%)]\tLoss: 0.594891\n",
      "====> Epoch: 1 Average loss: 0.0521\n",
      "Train Epoch: 2 [0/34 (0%)]\tLoss: 0.678528\n",
      "====> Epoch: 2 Average loss: 0.0459\n",
      "Train Epoch: 3 [0/34 (0%)]\tLoss: 0.479437\n",
      "====> Epoch: 3 Average loss: 0.0501\n",
      "Train Epoch: 4 [0/34 (0%)]\tLoss: 0.461799\n",
      "====> Epoch: 4 Average loss: 0.0332\n",
      "Train Epoch: 5 [0/34 (0%)]\tLoss: 0.373035\n",
      "====> Epoch: 5 Average loss: 0.0406\n",
      "Train Epoch: 6 [0/34 (0%)]\tLoss: 0.373514\n",
      "====> Epoch: 6 Average loss: 0.0265\n",
      "Train Epoch: 7 [0/34 (0%)]\tLoss: 0.291382\n",
      "====> Epoch: 7 Average loss: 0.0259\n",
      "Train Epoch: 8 [0/34 (0%)]\tLoss: 0.383160\n",
      "====> Epoch: 8 Average loss: 0.0249\n",
      "Train Epoch: 9 [0/34 (0%)]\tLoss: 0.196360\n",
      "====> Epoch: 9 Average loss: 0.0250\n",
      "Train Epoch: 10 [0/34 (0%)]\tLoss: 0.265015\n",
      "====> Epoch: 10 Average loss: 0.0189\n",
      "Train Epoch: 11 [0/34 (0%)]\tLoss: 0.152182\n",
      "====> Epoch: 11 Average loss: 0.0132\n",
      "Train Epoch: 12 [0/34 (0%)]\tLoss: 0.123265\n",
      "====> Epoch: 12 Average loss: 0.0124\n",
      "Train Epoch: 13 [0/34 (0%)]\tLoss: 0.114286\n",
      "====> Epoch: 13 Average loss: 0.0135\n",
      "Train Epoch: 14 [0/34 (0%)]\tLoss: 0.171488\n",
      "====> Epoch: 14 Average loss: 0.0105\n",
      "Train Epoch: 15 [0/34 (0%)]\tLoss: 0.154208\n",
      "====> Epoch: 15 Average loss: 0.0147\n",
      "Train Epoch: 16 [0/34 (0%)]\tLoss: 0.167369\n",
      "====> Epoch: 16 Average loss: 0.0179\n",
      "Train Epoch: 17 [0/34 (0%)]\tLoss: 0.121285\n",
      "====> Epoch: 17 Average loss: 0.0133\n",
      "Train Epoch: 18 [0/34 (0%)]\tLoss: 0.142611\n",
      "====> Epoch: 18 Average loss: 0.0112\n",
      "Train Epoch: 19 [0/34 (0%)]\tLoss: 0.086003\n",
      "====> Epoch: 19 Average loss: 0.0106\n",
      "Train Epoch: 20 [0/34 (0%)]\tLoss: 0.077178\n",
      "====> Epoch: 20 Average loss: 0.0118\n",
      "Train Epoch: 21 [0/34 (0%)]\tLoss: 0.103126\n",
      "====> Epoch: 21 Average loss: 0.0101\n",
      "Train Epoch: 22 [0/34 (0%)]\tLoss: 0.079666\n",
      "====> Epoch: 22 Average loss: 0.0123\n",
      "Train Epoch: 23 [0/34 (0%)]\tLoss: 0.070120\n",
      "====> Epoch: 23 Average loss: 0.0102\n",
      "Train Epoch: 24 [0/34 (0%)]\tLoss: 0.116217\n",
      "====> Epoch: 24 Average loss: 0.0096\n",
      "Train Epoch: 25 [0/34 (0%)]\tLoss: 0.115389\n",
      "====> Epoch: 25 Average loss: 0.0084\n",
      "Train Epoch: 26 [0/34 (0%)]\tLoss: 0.086228\n",
      "====> Epoch: 26 Average loss: 0.0101\n",
      "Train Epoch: 27 [0/34 (0%)]\tLoss: 0.090690\n",
      "====> Epoch: 27 Average loss: 0.0088\n",
      "Train Epoch: 28 [0/34 (0%)]\tLoss: 0.109976\n",
      "====> Epoch: 28 Average loss: 0.0077\n",
      "Train Epoch: 29 [0/34 (0%)]\tLoss: 0.062508\n",
      "====> Epoch: 29 Average loss: 0.0087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__: Iteration: 0: run prediction on unlabeled data `du` \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Initial precision: 0.7089039432789431 \n",
      "====> Initial recall: 0.7070942945942946 \n",
      "====> Initial f-score: 0.6880104379257193 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__: Update size of `dl`  and `du` by adding uncertain 60 samples in `dl` len(dl): 94, len(du) 314\n",
      "INFO:__main__: Update size of `dl`  and `du` by adding 0 hcs samples in `dl` len(dl): 94, len(du) 314\n",
      "INFO:__main__: Iteration: 0 fine-tune the model on dh U dl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/94 (0%)]\tLoss: 0.587193\n",
      "====> Epoch: 0 Average loss: 0.0332\n",
      "Train Epoch: 1 [0/94 (0%)]\tLoss: 0.374196\n",
      "====> Epoch: 1 Average loss: 0.0326\n",
      "Train Epoch: 2 [0/94 (0%)]\tLoss: 0.348885\n",
      "====> Epoch: 2 Average loss: 0.0299\n",
      "Train Epoch: 3 [0/94 (0%)]\tLoss: 0.475110\n",
      "====> Epoch: 3 Average loss: 0.0253\n",
      "Train Epoch: 4 [0/94 (0%)]\tLoss: 0.297162\n",
      "====> Epoch: 4 Average loss: 0.0227\n",
      "Train Epoch: 5 [0/94 (0%)]\tLoss: 0.423039\n",
      "====> Epoch: 5 Average loss: 0.0222\n",
      "Train Epoch: 6 [0/94 (0%)]\tLoss: 0.300604\n",
      "====> Epoch: 6 Average loss: 0.0218\n",
      "Train Epoch: 7 [0/94 (0%)]\tLoss: 0.313402\n",
      "====> Epoch: 7 Average loss: 0.0206\n",
      "Train Epoch: 8 [0/94 (0%)]\tLoss: 0.326467\n",
      "====> Epoch: 8 Average loss: 0.0175\n",
      "Train Epoch: 9 [0/94 (0%)]\tLoss: 0.236778\n",
      "====> Epoch: 9 Average loss: 0.0166\n",
      "Train Epoch: 10 [0/94 (0%)]\tLoss: 0.242976\n",
      "====> Epoch: 10 Average loss: 0.0154\n",
      "Train Epoch: 11 [0/94 (0%)]\tLoss: 0.306596\n",
      "====> Epoch: 11 Average loss: 0.0174\n",
      "Train Epoch: 12 [0/94 (0%)]\tLoss: 0.209299\n",
      "====> Epoch: 12 Average loss: 0.0175\n",
      "Train Epoch: 13 [0/94 (0%)]\tLoss: 0.155475\n",
      "====> Epoch: 13 Average loss: 0.0139\n",
      "Train Epoch: 14 [0/94 (0%)]\tLoss: 0.212180\n",
      "====> Epoch: 14 Average loss: 0.0161\n",
      "Train Epoch: 15 [0/94 (0%)]\tLoss: 0.231089\n",
      "====> Epoch: 15 Average loss: 0.0142\n",
      "Train Epoch: 16 [0/94 (0%)]\tLoss: 0.218714\n",
      "====> Epoch: 16 Average loss: 0.0141\n",
      "Train Epoch: 17 [0/94 (0%)]\tLoss: 0.171350\n",
      "====> Epoch: 17 Average loss: 0.0130\n",
      "Train Epoch: 18 [0/94 (0%)]\tLoss: 0.213908\n",
      "====> Epoch: 18 Average loss: 0.0132\n",
      "Train Epoch: 19 [0/94 (0%)]\tLoss: 0.274918\n",
      "====> Epoch: 19 Average loss: 0.0138\n",
      "Train Epoch: 20 [0/94 (0%)]\tLoss: 0.243506\n",
      "====> Epoch: 20 Average loss: 0.0137\n",
      "Train Epoch: 21 [0/94 (0%)]\tLoss: 0.208497\n",
      "====> Epoch: 21 Average loss: 0.0130\n",
      "Train Epoch: 22 [0/94 (0%)]\tLoss: 0.109280\n",
      "====> Epoch: 22 Average loss: 0.0112\n",
      "Train Epoch: 23 [0/94 (0%)]\tLoss: 0.169673\n",
      "====> Epoch: 23 Average loss: 0.0120\n",
      "Train Epoch: 24 [0/94 (0%)]\tLoss: 0.187086\n",
      "====> Epoch: 24 Average loss: 0.0121\n",
      "Train Epoch: 25 [0/94 (0%)]\tLoss: 0.214436\n",
      "====> Epoch: 25 Average loss: 0.0119\n",
      "Train Epoch: 26 [0/94 (0%)]\tLoss: 0.266820\n",
      "====> Epoch: 26 Average loss: 0.0117\n",
      "Train Epoch: 27 [0/94 (0%)]\tLoss: 0.166929\n",
      "====> Epoch: 27 Average loss: 0.0125\n",
      "Train Epoch: 28 [0/94 (0%)]\tLoss: 0.171851\n",
      "====> Epoch: 28 Average loss: 0.0111\n",
      "Train Epoch: 29 [0/94 (0%)]\tLoss: 0.187418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__: remove 60 uncertain samples from du\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 29 Average loss: 0.0120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__: Iteration: 1: run prediction on unlabeled data `du` \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, len(dl): 94, len(du): 254, len(dh) 0\n",
      "Precision: 0.6942364637677138\n",
      "Recall: 0.6926780395530395\n",
      "F1: 0.6779382829492924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__: Update size of `dl`  and `du` by adding uncertain 60 samples in `dl` len(dl): 154, len(du) 254\n",
      "INFO:__main__: Update size of `dl`  and `du` by adding 0 hcs samples in `dl` len(dl): 154, len(du) 254\n",
      "INFO:__main__: Iteration: 1 fine-tune the model on dh U dl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/154 (0%)]\tLoss: 0.203708\n",
      "====> Epoch: 0 Average loss: 0.0212\n",
      "Train Epoch: 1 [0/154 (0%)]\tLoss: 0.235874\n",
      "====> Epoch: 1 Average loss: 0.0191\n",
      "Train Epoch: 2 [0/154 (0%)]\tLoss: 0.340939\n",
      "====> Epoch: 2 Average loss: 0.0172\n",
      "Train Epoch: 3 [0/154 (0%)]\tLoss: 0.411711\n",
      "====> Epoch: 3 Average loss: 0.0179\n",
      "Train Epoch: 4 [0/154 (0%)]\tLoss: 0.265548\n",
      "====> Epoch: 4 Average loss: 0.0177\n",
      "Train Epoch: 5 [0/154 (0%)]\tLoss: 0.305452\n",
      "====> Epoch: 5 Average loss: 0.0174\n",
      "Train Epoch: 6 [0/154 (0%)]\tLoss: 0.224350\n",
      "====> Epoch: 6 Average loss: 0.0171\n",
      "Train Epoch: 7 [0/154 (0%)]\tLoss: 0.173309\n",
      "====> Epoch: 7 Average loss: 0.0147\n",
      "Train Epoch: 8 [0/154 (0%)]\tLoss: 0.150634\n",
      "====> Epoch: 8 Average loss: 0.0155\n",
      "Train Epoch: 9 [0/154 (0%)]\tLoss: 0.277503\n",
      "====> Epoch: 9 Average loss: 0.0152\n",
      "Train Epoch: 10 [0/154 (0%)]\tLoss: 0.244627\n",
      "====> Epoch: 10 Average loss: 0.0134\n",
      "Train Epoch: 11 [0/154 (0%)]\tLoss: 0.265676\n",
      "====> Epoch: 11 Average loss: 0.0152\n",
      "Train Epoch: 12 [0/154 (0%)]\tLoss: 0.205187\n",
      "====> Epoch: 12 Average loss: 0.0148\n",
      "Train Epoch: 13 [0/154 (0%)]\tLoss: 0.329672\n",
      "====> Epoch: 13 Average loss: 0.0139\n",
      "Train Epoch: 14 [0/154 (0%)]\tLoss: 0.164641\n",
      "====> Epoch: 14 Average loss: 0.0133\n",
      "Train Epoch: 15 [0/154 (0%)]\tLoss: 0.234670\n",
      "====> Epoch: 15 Average loss: 0.0132\n",
      "Train Epoch: 16 [0/154 (0%)]\tLoss: 0.088587\n",
      "====> Epoch: 16 Average loss: 0.0128\n",
      "Train Epoch: 17 [0/154 (0%)]\tLoss: 0.142347\n",
      "====> Epoch: 17 Average loss: 0.0128\n",
      "Train Epoch: 18 [0/154 (0%)]\tLoss: 0.174205\n",
      "====> Epoch: 18 Average loss: 0.0117\n",
      "Train Epoch: 19 [0/154 (0%)]\tLoss: 0.179988\n",
      "====> Epoch: 19 Average loss: 0.0112\n",
      "Train Epoch: 20 [0/154 (0%)]\tLoss: 0.128013\n",
      "====> Epoch: 20 Average loss: 0.0124\n",
      "Train Epoch: 21 [0/154 (0%)]\tLoss: 0.137133\n",
      "====> Epoch: 21 Average loss: 0.0124\n",
      "Train Epoch: 22 [0/154 (0%)]\tLoss: 0.259451\n",
      "====> Epoch: 22 Average loss: 0.0134\n",
      "Train Epoch: 23 [0/154 (0%)]\tLoss: 0.249523\n",
      "====> Epoch: 23 Average loss: 0.0133\n",
      "Train Epoch: 24 [0/154 (0%)]\tLoss: 0.170880\n",
      "====> Epoch: 24 Average loss: 0.0126\n",
      "Train Epoch: 25 [0/154 (0%)]\tLoss: 0.108499\n",
      "====> Epoch: 25 Average loss: 0.0119\n",
      "Train Epoch: 26 [0/154 (0%)]\tLoss: 0.243034\n",
      "====> Epoch: 26 Average loss: 0.0126\n",
      "Train Epoch: 27 [0/154 (0%)]\tLoss: 0.189790\n",
      "====> Epoch: 27 Average loss: 0.0121\n",
      "Train Epoch: 28 [0/154 (0%)]\tLoss: 0.191320\n",
      "====> Epoch: 28 Average loss: 0.0116\n",
      "Train Epoch: 29 [0/154 (0%)]\tLoss: 0.155729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__: remove 60 uncertain samples from du\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 29 Average loss: 0.0118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__: Iteration: 2: run prediction on unlabeled data `du` \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1, len(dl): 154, len(du): 194, len(dh) 0\n",
      "Precision: 0.731528511997262\n",
      "Recall: 0.7073545667295666\n",
      "F1: 0.696806800071029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__: Update size of `dl`  and `du` by adding uncertain 60 samples in `dl` len(dl): 214, len(du) 194\n",
      "INFO:__main__: Update size of `dl`  and `du` by adding 0 hcs samples in `dl` len(dl): 214, len(du) 194\n",
      "INFO:__main__: Iteration: 2 fine-tune the model on dh U dl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/214 (0%)]\tLoss: 0.295885\n",
      "====> Epoch: 0 Average loss: 0.0234\n",
      "Train Epoch: 1 [0/214 (0%)]\tLoss: 0.303193\n",
      "====> Epoch: 1 Average loss: 0.0199\n",
      "Train Epoch: 2 [0/214 (0%)]\tLoss: 0.291385\n",
      "====> Epoch: 2 Average loss: 0.0187\n",
      "Train Epoch: 3 [0/214 (0%)]\tLoss: 0.249911\n",
      "====> Epoch: 3 Average loss: 0.0180\n",
      "Train Epoch: 4 [0/214 (0%)]\tLoss: 0.174258\n",
      "====> Epoch: 4 Average loss: 0.0178\n",
      "Train Epoch: 5 [0/214 (0%)]\tLoss: 0.339058\n",
      "====> Epoch: 5 Average loss: 0.0171\n",
      "Train Epoch: 6 [0/214 (0%)]\tLoss: 0.275777\n",
      "====> Epoch: 6 Average loss: 0.0186\n",
      "Train Epoch: 7 [0/214 (0%)]\tLoss: 0.378364\n",
      "====> Epoch: 7 Average loss: 0.0180\n",
      "Train Epoch: 8 [0/214 (0%)]\tLoss: 0.206786\n",
      "====> Epoch: 8 Average loss: 0.0170\n",
      "Train Epoch: 9 [0/214 (0%)]\tLoss: 0.374637\n",
      "====> Epoch: 9 Average loss: 0.0172\n",
      "Train Epoch: 10 [0/214 (0%)]\tLoss: 0.297408\n",
      "====> Epoch: 10 Average loss: 0.0163\n",
      "Train Epoch: 11 [0/214 (0%)]\tLoss: 0.206912\n",
      "====> Epoch: 11 Average loss: 0.0160\n",
      "Train Epoch: 12 [0/214 (0%)]\tLoss: 0.274844\n",
      "====> Epoch: 12 Average loss: 0.0148\n",
      "Train Epoch: 13 [0/214 (0%)]\tLoss: 0.130124\n",
      "====> Epoch: 13 Average loss: 0.0156\n",
      "Train Epoch: 14 [0/214 (0%)]\tLoss: 0.313766\n",
      "====> Epoch: 14 Average loss: 0.0170\n",
      "Train Epoch: 15 [0/214 (0%)]\tLoss: 0.248076\n",
      "====> Epoch: 15 Average loss: 0.0132\n",
      "Train Epoch: 16 [0/214 (0%)]\tLoss: 0.191248\n",
      "====> Epoch: 16 Average loss: 0.0142\n",
      "Train Epoch: 17 [0/214 (0%)]\tLoss: 0.200669\n",
      "====> Epoch: 17 Average loss: 0.0137\n",
      "Train Epoch: 18 [0/214 (0%)]\tLoss: 0.383898\n",
      "====> Epoch: 18 Average loss: 0.0146\n",
      "Train Epoch: 19 [0/214 (0%)]\tLoss: 0.297741\n",
      "====> Epoch: 19 Average loss: 0.0131\n",
      "Train Epoch: 20 [0/214 (0%)]\tLoss: 0.209813\n",
      "====> Epoch: 20 Average loss: 0.0140\n",
      "Train Epoch: 21 [0/214 (0%)]\tLoss: 0.291606\n",
      "====> Epoch: 21 Average loss: 0.0141\n",
      "Train Epoch: 22 [0/214 (0%)]\tLoss: 0.183689\n",
      "====> Epoch: 22 Average loss: 0.0141\n",
      "Train Epoch: 23 [0/214 (0%)]\tLoss: 0.133050\n",
      "====> Epoch: 23 Average loss: 0.0126\n",
      "Train Epoch: 24 [0/214 (0%)]\tLoss: 0.384731\n",
      "====> Epoch: 24 Average loss: 0.0137\n",
      "Train Epoch: 25 [0/214 (0%)]\tLoss: 0.183853\n",
      "====> Epoch: 25 Average loss: 0.0117\n",
      "Train Epoch: 26 [0/214 (0%)]\tLoss: 0.197317\n",
      "====> Epoch: 26 Average loss: 0.0132\n",
      "Train Epoch: 27 [0/214 (0%)]\tLoss: 0.238185\n",
      "====> Epoch: 27 Average loss: 0.0125\n",
      "Train Epoch: 28 [0/214 (0%)]\tLoss: 0.134783\n",
      "====> Epoch: 28 Average loss: 0.0131\n",
      "Train Epoch: 29 [0/214 (0%)]\tLoss: 0.313864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__: remove 60 uncertain samples from du\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 29 Average loss: 0.0123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__: Iteration: 3: run prediction on unlabeled data `du` \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2, len(dl): 214, len(du): 134, len(dh) 0\n",
      "Precision: 0.7505345349095349\n",
      "Recall: 0.7110879571817073\n",
      "F1: 0.7068258839923232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__: Update size of `dl`  and `du` by adding uncertain 60 samples in `dl` len(dl): 274, len(du) 134\n",
      "INFO:__main__: Update size of `dl`  and `du` by adding 0 hcs samples in `dl` len(dl): 274, len(du) 134\n",
      "INFO:__main__: Iteration: 3 fine-tune the model on dh U dl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/274 (0%)]\tLoss: 0.194941\n",
      "====> Epoch: 0 Average loss: 0.0187\n",
      "Train Epoch: 1 [0/274 (0%)]\tLoss: 0.259124\n",
      "====> Epoch: 1 Average loss: 0.0182\n",
      "Train Epoch: 2 [0/274 (0%)]\tLoss: 0.396914\n",
      "====> Epoch: 2 Average loss: 0.0194\n",
      "Train Epoch: 3 [0/274 (0%)]\tLoss: 0.276273\n",
      "====> Epoch: 3 Average loss: 0.0201\n",
      "Train Epoch: 4 [0/274 (0%)]\tLoss: 0.185078\n",
      "====> Epoch: 4 Average loss: 0.0170\n",
      "Train Epoch: 5 [0/274 (0%)]\tLoss: 0.309386\n",
      "====> Epoch: 5 Average loss: 0.0176\n",
      "Train Epoch: 6 [0/274 (0%)]\tLoss: 0.098197\n",
      "====> Epoch: 6 Average loss: 0.0149\n",
      "Train Epoch: 7 [0/274 (0%)]\tLoss: 0.206181\n",
      "====> Epoch: 7 Average loss: 0.0168\n",
      "Train Epoch: 8 [0/274 (0%)]\tLoss: 0.347081\n",
      "====> Epoch: 8 Average loss: 0.0176\n",
      "Train Epoch: 9 [0/274 (0%)]\tLoss: 0.190324\n",
      "====> Epoch: 9 Average loss: 0.0156\n",
      "Train Epoch: 10 [0/274 (0%)]\tLoss: 0.187592\n",
      "====> Epoch: 10 Average loss: 0.0174\n",
      "Train Epoch: 11 [0/274 (0%)]\tLoss: 0.167435\n",
      "====> Epoch: 11 Average loss: 0.0152\n",
      "Train Epoch: 12 [0/274 (0%)]\tLoss: 0.198419\n",
      "====> Epoch: 12 Average loss: 0.0148\n",
      "Train Epoch: 13 [0/274 (0%)]\tLoss: 0.264020\n",
      "====> Epoch: 13 Average loss: 0.0162\n",
      "Train Epoch: 14 [0/274 (0%)]\tLoss: 0.111169\n",
      "====> Epoch: 14 Average loss: 0.0157\n",
      "Train Epoch: 15 [0/274 (0%)]\tLoss: 0.247211\n",
      "====> Epoch: 15 Average loss: 0.0143\n",
      "Train Epoch: 16 [0/274 (0%)]\tLoss: 0.234991\n",
      "====> Epoch: 16 Average loss: 0.0167\n",
      "Train Epoch: 17 [0/274 (0%)]\tLoss: 0.229863\n",
      "====> Epoch: 17 Average loss: 0.0140\n",
      "Train Epoch: 18 [0/274 (0%)]\tLoss: 0.252661\n",
      "====> Epoch: 18 Average loss: 0.0138\n",
      "Train Epoch: 19 [0/274 (0%)]\tLoss: 0.233488\n",
      "====> Epoch: 19 Average loss: 0.0120\n",
      "Train Epoch: 20 [0/274 (0%)]\tLoss: 0.189513\n",
      "====> Epoch: 20 Average loss: 0.0128\n",
      "Train Epoch: 21 [0/274 (0%)]\tLoss: 0.306677\n",
      "====> Epoch: 21 Average loss: 0.0146\n",
      "Train Epoch: 22 [0/274 (0%)]\tLoss: 0.219250\n",
      "====> Epoch: 22 Average loss: 0.0135\n",
      "Train Epoch: 23 [0/274 (0%)]\tLoss: 0.212759\n",
      "====> Epoch: 23 Average loss: 0.0122\n",
      "Train Epoch: 24 [0/274 (0%)]\tLoss: 0.221650\n",
      "====> Epoch: 24 Average loss: 0.0141\n",
      "Train Epoch: 25 [0/274 (0%)]\tLoss: 0.237560\n",
      "====> Epoch: 25 Average loss: 0.0133\n",
      "Train Epoch: 26 [0/274 (0%)]\tLoss: 0.170092\n",
      "====> Epoch: 26 Average loss: 0.0139\n",
      "Train Epoch: 27 [0/274 (0%)]\tLoss: 0.327057\n",
      "====> Epoch: 27 Average loss: 0.0122\n",
      "Train Epoch: 28 [0/274 (0%)]\tLoss: 0.177231\n",
      "====> Epoch: 28 Average loss: 0.0156\n",
      "Train Epoch: 29 [0/274 (0%)]\tLoss: 0.138410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__: remove 60 uncertain samples from du\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 29 Average loss: 0.0138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__: Iteration: 4: run prediction on unlabeled data `du` \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3, len(dl): 274, len(du): 74, len(dh) 0\n",
      "Precision: 0.6859481375106375\n",
      "Recall: 0.6854937597125097\n",
      "F1: 0.6669745944034061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__: Update size of `dl`  and `du` by adding uncertain 60 samples in `dl` len(dl): 334, len(du) 74\n",
      "INFO:__main__: Update size of `dl`  and `du` by adding 0 hcs samples in `dl` len(dl): 334, len(du) 74\n",
      "INFO:__main__: Iteration: 4 fine-tune the model on dh U dl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/334 (0%)]\tLoss: 0.333235\n",
      "====> Epoch: 0 Average loss: 0.0169\n",
      "Train Epoch: 1 [0/334 (0%)]\tLoss: 0.249951\n",
      "====> Epoch: 1 Average loss: 0.0159\n",
      "Train Epoch: 2 [0/334 (0%)]\tLoss: 0.233464\n",
      "====> Epoch: 2 Average loss: 0.0166\n",
      "Train Epoch: 3 [0/334 (0%)]\tLoss: 0.160521\n",
      "====> Epoch: 3 Average loss: 0.0143\n",
      "Train Epoch: 4 [0/334 (0%)]\tLoss: 0.373476\n",
      "====> Epoch: 4 Average loss: 0.0153\n",
      "Train Epoch: 5 [0/334 (0%)]\tLoss: 0.230807\n",
      "====> Epoch: 5 Average loss: 0.0149\n",
      "Train Epoch: 6 [0/334 (0%)]\tLoss: 0.146505\n",
      "====> Epoch: 6 Average loss: 0.0138\n",
      "Train Epoch: 7 [0/334 (0%)]\tLoss: 0.177746\n",
      "====> Epoch: 7 Average loss: 0.0145\n",
      "Train Epoch: 8 [0/334 (0%)]\tLoss: 0.221866\n",
      "====> Epoch: 8 Average loss: 0.0136\n",
      "Train Epoch: 9 [0/334 (0%)]\tLoss: 0.183206\n",
      "====> Epoch: 9 Average loss: 0.0131\n",
      "Train Epoch: 10 [0/334 (0%)]\tLoss: 0.376792\n",
      "====> Epoch: 10 Average loss: 0.0124\n",
      "Train Epoch: 11 [0/334 (0%)]\tLoss: 0.188778\n",
      "====> Epoch: 11 Average loss: 0.0140\n",
      "Train Epoch: 12 [0/334 (0%)]\tLoss: 0.118978\n",
      "====> Epoch: 12 Average loss: 0.0129\n",
      "Train Epoch: 13 [0/334 (0%)]\tLoss: 0.143951\n",
      "====> Epoch: 13 Average loss: 0.0113\n",
      "Train Epoch: 14 [0/334 (0%)]\tLoss: 0.176952\n",
      "====> Epoch: 14 Average loss: 0.0128\n",
      "Train Epoch: 15 [0/334 (0%)]\tLoss: 0.230575\n",
      "====> Epoch: 15 Average loss: 0.0129\n",
      "Train Epoch: 16 [0/334 (0%)]\tLoss: 0.157648\n",
      "====> Epoch: 16 Average loss: 0.0138\n",
      "Train Epoch: 17 [0/334 (0%)]\tLoss: 0.212291\n",
      "====> Epoch: 17 Average loss: 0.0140\n",
      "Train Epoch: 18 [0/334 (0%)]\tLoss: 0.148687\n",
      "====> Epoch: 18 Average loss: 0.0124\n",
      "Train Epoch: 19 [0/334 (0%)]\tLoss: 0.174227\n",
      "====> Epoch: 19 Average loss: 0.0134\n",
      "Train Epoch: 20 [0/334 (0%)]\tLoss: 0.217964\n",
      "====> Epoch: 20 Average loss: 0.0126\n",
      "Train Epoch: 21 [0/334 (0%)]\tLoss: 0.153917\n",
      "====> Epoch: 21 Average loss: 0.0137\n",
      "Train Epoch: 22 [0/334 (0%)]\tLoss: 0.136228\n",
      "====> Epoch: 22 Average loss: 0.0120\n",
      "Train Epoch: 23 [0/334 (0%)]\tLoss: 0.211876\n",
      "====> Epoch: 23 Average loss: 0.0110\n",
      "Train Epoch: 24 [0/334 (0%)]\tLoss: 0.134444\n",
      "====> Epoch: 24 Average loss: 0.0120\n",
      "Train Epoch: 25 [0/334 (0%)]\tLoss: 0.117798\n",
      "====> Epoch: 25 Average loss: 0.0131\n",
      "Train Epoch: 26 [0/334 (0%)]\tLoss: 0.366330\n",
      "====> Epoch: 26 Average loss: 0.0134\n",
      "Train Epoch: 27 [0/334 (0%)]\tLoss: 0.279490\n",
      "====> Epoch: 27 Average loss: 0.0130\n",
      "Train Epoch: 28 [0/334 (0%)]\tLoss: 0.165007\n",
      "====> Epoch: 28 Average loss: 0.0136\n",
      "Train Epoch: 29 [0/334 (0%)]\tLoss: 0.122569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__: remove 60 uncertain samples from du\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 29 Average loss: 0.0121\n",
      "Iteration: 4, len(dl): 334, len(du): 14, len(dh) 0\n",
      "Precision: 0.7328382554945055\n",
      "Recall: 0.7235808173308174\n",
      "F1: 0.7097713661123549\n",
      "Saving model...\n",
      "Model saved as: saved_model/alexnet.pth\n"
     ]
    }
   ],
   "source": [
    "dataset_train = GameImageDataset(\n",
    "    root_dir=\"data/train\",\n",
    "    transform=transforms.Compose(\n",
    "        [SquarifyImage(),\n",
    "         RandomCrop(224),\n",
    "         Normalize(),\n",
    "         ToTensor()]))\n",
    "\n",
    "dataset_test = GameImageDataset(\n",
    "    root_dir=\"data/test\",\n",
    "    transform=transforms.Compose(\n",
    "        [SquarifyImage(),\n",
    "         RandomCrop(224),\n",
    "         Normalize(),\n",
    "         ToTensor()]))\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "random_seed = 123\n",
    "validation_split = 0.1  # 10%\n",
    "shuffling_dataset = True\n",
    "batch_size = 16\n",
    "dataset_size = len(dataset_train)\n",
    "\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "\n",
    "if shuffling_dataset:\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "du = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size,\n",
    "                                 sampler=train_sampler, num_workers=1)\n",
    "dl = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size,\n",
    "                                 sampler=valid_sampler, num_workers=1)\n",
    "dtest = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size,\n",
    "                                    num_workers=1)\n",
    "\n",
    "ceal_learning_algorithm(du=du, dl=dl, dtest=dtest, k=60, max_iter=5, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra processing\n",
    "---\n",
    "#### The cells are converted to markdown cells so that they are not run when you run all cells. Change them to code cells to run if required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting data zip file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('data/annotated_r2.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Move files to train and test directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import os\n",
    "dataitems = os.listdir('data')\n",
    "finalitems = []\n",
    "for item in dataitems:\n",
    "    if not os.path.isdir('data/'+item):\n",
    "        finalitems.append(item)\n",
    "\n",
    "for idx, item in enumerate(finalitems):\n",
    "    # this will assign 20% images(every 5th image) to the test directory\n",
    "    if idx % 5 == 0: \n",
    "        os.rename('data/' + item, 'data/test/'+item)\n",
    "    else:\n",
    "        os.rename('data/' + item, 'data/train/'+item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create small train-test set for debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import os\n",
    "from shutil import copyfile\n",
    "\n",
    "trainitems = os.listdir('data/train')\n",
    "testitems = os.listdir('data/test')\n",
    "\n",
    "for idx, item in enumerate(trainitems):\n",
    "    if os.path.isdir('data/train/'+item):\n",
    "        continue\n",
    "        \n",
    "    # this will assign 20% images(every 5th image) to the debug directory\n",
    "    if idx % 5 == 0: \n",
    "        copyfile('data/train/'+item,'data/debug_train/'+item)\n",
    "        \n",
    "for idx, item in enumerate(testitems):\n",
    "    if os.path.isdir('data/train/'+item):\n",
    "        continue\n",
    "        \n",
    "    # this will assign 20% images(every 5th image) to the debug directory\n",
    "    if idx % 5 == 0: \n",
    "        copyfile('data/test/'+item,'data/debug_test/'+item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
