{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code  \n",
    "---\n",
    "This code is taken from: https://github.com/rafikg/CEAL\n",
    "## Import all the required stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imported from run_ceal/ceal_learning_algorithm.py\n",
    "from utils import Normalize, RandomCrop, SquarifyImage, \\\n",
    "    ToTensor\n",
    "from utils import get_uncertain_samples, get_high_confidence_samples, \\\n",
    "    update_threshold\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "\n",
    "# imported from model/alexnet.py\n",
    "from typing import Optional, Callable\n",
    "\n",
    "from torchvision.models import alexnet\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.functional import softmax\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.optim as Optimizer\n",
    "import logging\n",
    "\n",
    "\n",
    "# imported from utils/dataset.py\n",
    "from typing import Optional, Callable, Union\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import warnings\n",
    "import cv2\n",
    "\n",
    "#imported by me\n",
    "from sklearn.metrics import precision_recall_fscore_support as prfs\n",
    "\n",
    "logging.basicConfig(format=\"%(levelname)s:%(name)s: %(message)s\",\n",
    "                    level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format=\"%(levelname)s:%(name)s: %(message)s\",\n",
    "                    level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class AlexNet(object):\n",
    "    \"\"\"\n",
    "    Encapsulate the pretrained alexnet model\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_classes : int, default(256)\n",
    "        the new number of classes\n",
    "    device: Optional[str] 'cuda' or 'cpu', default(None)\n",
    "            if None: cuda will be used if it is available\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_classes: int = 4, device: Optional[str] = None):\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "        self.model = alexnet(pretrained=True, progress=True)\n",
    "\n",
    "        self.__freeze_all_layers()\n",
    "        self.__change_last_layer()\n",
    "        if device is None:\n",
    "            self.device = torch.device(\n",
    "                \"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        logger.info('The code is running on {} '.format(self.device))\n",
    "\n",
    "    def __freeze_all_layers(self) -> None:\n",
    "        \"\"\"\n",
    "        freeze all layers in alexnet\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def __change_last_layer(self) -> None:\n",
    "        \"\"\"\n",
    "        change last layer to accept n_classes instead of 1000 classes\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        self.model.classifier[6] = nn.Linear(4096, self.n_classes)\n",
    "\n",
    "    def __add_softmax_layer(self) -> None:\n",
    "        \"\"\"\n",
    "        Add softmax layer to alexnet model\n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "        \"\"\"\n",
    "        # add softmax layer\n",
    "        self.model = nn.Sequential(self.model, nn.LogSoftmax(dim=1))\n",
    "\n",
    "    def __train_one_epoch(self, train_loader: DataLoader,\n",
    "                          optimizer: Optimizer,\n",
    "                          criterion: Callable,\n",
    "                          valid_loader: DataLoader = None,\n",
    "                          epoch: int = 0,\n",
    "                          each_batch_idx: int = 300) -> None:\n",
    "        \"\"\"\n",
    "        Train alexnet for one epoch\n",
    "        Parameters\n",
    "        ----------\n",
    "        train_loader : DataLoader\n",
    "        criterion :  Callable\n",
    "        optimizer : Optimizer (torch.optim)\n",
    "        epoch : int\n",
    "        each_batch_idx : int\n",
    "            print training stats after each_batch_idx\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        train_loss = 0\n",
    "        data_size = 0\n",
    "\n",
    "        for batch_idx, sample_batched in enumerate(train_loader):\n",
    "            # load data and label\n",
    "            data, label = sample_batched['image'], sample_batched['label']\n",
    "\n",
    "            # convert data and label to be compatible with the device\n",
    "            data = data.to(self.device)\n",
    "            data = data.float()\n",
    "            label = label.to(self.device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # run forward\n",
    "            pred_prob = self.model(data)\n",
    "\n",
    "            # calculate loss\n",
    "            label = label.float()\n",
    "            loss = criterion(pred_prob, label)\n",
    "\n",
    "            # calculate gradient (backprop)\n",
    "            loss.backward()\n",
    "\n",
    "            # total train loss\n",
    "            train_loss += loss.item()\n",
    "            data_size += label.size(0)\n",
    "\n",
    "            # update weights\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % each_batch_idx == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data),\n",
    "                    len(train_loader.sampler.indices),\n",
    "                    100. * batch_idx / len(train_loader.sampler.indices),\n",
    "                    loss.item()))\n",
    "        if valid_loader:\n",
    "            p, r, f = self.evaluate(test_loader=valid_loader)\n",
    "            print('Precision on the valid dataset {}'.format(p))\n",
    "            print('Recall on the valid dataset {}'.format(r))\n",
    "            print('F-score on the valid dataset {}'.format(f))\n",
    "\n",
    "        print('====> Epoch: {} Average loss: {:.4f}'.\n",
    "              format(epoch,\n",
    "                     train_loss / data_size))\n",
    "\n",
    "    def train(self, epochs: int, train_loader: DataLoader,\n",
    "              valid_loader: DataLoader = None) -> None:\n",
    "        \"\"\"\n",
    "        Train alexnet for several epochs\n",
    "        Parameters\n",
    "        ----------\n",
    "        epochs : int\n",
    "            number of epochs\n",
    "        train_loader:  DataLoader\n",
    "            training set\n",
    "        valid_loader : DataLoader, Optional\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        self.model.to(self.device)\n",
    "        self.model.train()\n",
    "        optimizer = optim.SGD(\n",
    "            filter(lambda p: p.requires_grad, self.model.parameters()),\n",
    "            lr=0.001, momentum=0.9)\n",
    "\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        for epoch in range(epochs):\n",
    "            self.__train_one_epoch(train_loader=train_loader,\n",
    "                                   optimizer=optimizer,\n",
    "                                   criterion=criterion,\n",
    "                                   valid_loader=valid_loader,\n",
    "                                   epoch=epoch\n",
    "                                   )\n",
    "\n",
    "    def evaluate(self, test_loader: DataLoader) -> float:\n",
    "        \"\"\"\n",
    "        Calaculate alexnet accuracy on test data\n",
    "        Parameters\n",
    "        ----------\n",
    "        test_loader: DataLoader\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        accuracy: float\n",
    "        \"\"\"\n",
    "        precisions = [0,0,0,0]\n",
    "        recalls = [0,0,0,0]\n",
    "        fscores = [0,0,0,0]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, sample_batched in enumerate(test_loader):\n",
    "                data, labels = sample_batched['image'], \\\n",
    "                               sample_batched['label']\n",
    "                data = data.to(self.device)\n",
    "                data = data.float()\n",
    "                labels = labels.to(self.device)\n",
    "                outputs = self.model(data)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                outputs = torch.sigmoid(outputs)\n",
    "                \n",
    "                for i in range(4):\n",
    "                    p, r, f, _ = prfs(labels[:,i].cpu().numpy(),outputs[:,i].cpu().numpy()>0.5, average='binary')\n",
    "                    precisions[i] += p\n",
    "                    recalls[i] += r\n",
    "                    fscores[i] += f\n",
    "                \n",
    "        return precisions, recalls, fscores\n",
    "\n",
    "    def predict(self, test_loader):\n",
    "        \"\"\"\n",
    "        Run the inference pipeline on the test_loader data\n",
    "        Parameters\n",
    "        ----------\n",
    "        test_loader: DataLoader\n",
    "            test data\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        self.model.to(self.device)\n",
    "        predict_results = np.empty(shape=(0, 4))\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, sample_batched in enumerate(test_loader):\n",
    "                data, _ = sample_batched['image'], \\\n",
    "                          sample_batched['label']\n",
    "                data = data.to(self.device)\n",
    "                data = data.float()\n",
    "                outputs = self.model(data)\n",
    "                outputs = softmax(outputs)\n",
    "                predict_results = np.concatenate(\n",
    "                    (predict_results, outputs.cpu().numpy()))\n",
    "        return predict_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make custom dataset for the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "class GameImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Encapsulate Caltech256 torch.utils.data.Dataset\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    root_dir : str\n",
    "        Path to the dataset directory.\n",
    "\n",
    "    transform : Callable,\n",
    "        A transform function that takes the original image and\n",
    "        return a transformed version.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    data : list\n",
    "        list of images files names\n",
    "    labels : list\n",
    "        list of integers (labels)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir: str = \"data\",\n",
    "                 transform: Optional[Callable] = None):\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self._classes = 4\n",
    "\n",
    "        # load data and labels\n",
    "        for item in os.listdir(root_dir):\n",
    "            filepath = os.path.join(root_dir, item)\n",
    "            if not os.path.isdir(filepath):\n",
    "                # read and process image\n",
    "                img = cv2.imread(filepath)\n",
    "                img = img[:, :, ::-1]\n",
    "                img = self.img_normalize(img)\n",
    "                self.data.append(img)\n",
    "                \n",
    "                # read labels from filename\n",
    "                label = item.split('.')[0][-4:]\n",
    "                label = [int(i) for i in label]\n",
    "                self.labels.append(label)\n",
    "                \n",
    "\n",
    "    def __getitem__(self, idx: int) -> dict:\n",
    "        \"\"\"\n",
    "        Get the idx element\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx : int\n",
    "           the index of the element\n",
    "\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        sample: dict[str, Any]\n",
    "        \"\"\"\n",
    "        \n",
    "        sample = {'image': self.data[idx], 'label': self.labels[idx]}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.data)\n",
    "\n",
    "    @staticmethod\n",
    "    def img_normalize(img):\n",
    "        img = (img / 255.0)\n",
    "\n",
    "        return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CEAL Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ceal_learning_algorithm(du: DataLoader,\n",
    "                            dl: DataLoader,\n",
    "                            dtest: DataLoader,\n",
    "                            k: int = 5,\n",
    "                            delta_0: float = 0.005,\n",
    "                            dr: float = 0.00033,\n",
    "                            t: int = 1,\n",
    "                            epochs: int = 10,\n",
    "                            criteria: str = 'cl',\n",
    "                            max_iter: int = 10):\n",
    "    \"\"\"\n",
    "    Algorithm1 : Learning algorithm of CEAL.\n",
    "    For simplicity, I used the same notation in the paper.\n",
    "    Parameters\n",
    "    ----------\n",
    "    du: DataLoader\n",
    "        Unlabeled samples\n",
    "    dl : DataLoader\n",
    "        labeled samples\n",
    "    dtest : DataLoader\n",
    "        test data\n",
    "    k: int, (default = 1000)\n",
    "        uncertain samples selection\n",
    "    delta_0: float\n",
    "        hight confidence samples selection threshold\n",
    "    dr: float\n",
    "        threshold decay\n",
    "    t: int\n",
    "        fine-tuning interval\n",
    "    epochs: int\n",
    "    criteria: str\n",
    "    max_iter: int\n",
    "        maximum iteration number.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    \"\"\"\n",
    "    logger.info('Initial configuration: len(du): {}, len(dl): {} '.format(\n",
    "        len(du.sampler.indices),\n",
    "        len(dl.sampler.indices)))\n",
    "\n",
    "    # Create the model\n",
    "    model = AlexNet(n_classes=4, device=device)\n",
    "\n",
    "    # Initialize the model\n",
    "    logger.info('Intialize training the model on `dl` and test on `dtest`')\n",
    "\n",
    "    model.train(epochs=epochs, train_loader=dl, valid_loader=None)\n",
    "\n",
    "    # Evaluate model on dtest\n",
    "    p, r, f = model.evaluate(test_loader=dtest)\n",
    "\n",
    "    print('====> Initial precision: {} '.format(p))\n",
    "    print('====> Initial recall: {} '.format(r))\n",
    "    print('====> Initial f-score: {} '.format(f))\n",
    "\n",
    "    for iteration in range(max_iter):\n",
    "\n",
    "        logger.info('Iteration: {}: run prediction on unlabeled data '\n",
    "                    '`du` '.format(iteration))\n",
    "\n",
    "        pred_prob = model.predict(test_loader=du)\n",
    "\n",
    "        # get k uncertain samples\n",
    "        uncert_samp_idx, _ = get_uncertain_samples(pred_prob=pred_prob, k=k,\n",
    "                                                   criteria=criteria)\n",
    "\n",
    "        # get original indices\n",
    "        uncert_samp_idx = [du.sampler.indices[idx] for idx in uncert_samp_idx]\n",
    "\n",
    "        # add the uncertain samples selected from `du` to the labeled samples\n",
    "        #  set `dl`\n",
    "        dl.sampler.indices.extend(uncert_samp_idx)\n",
    "\n",
    "        logger.info(\n",
    "            'Update size of `dl`  and `du` by adding uncertain {} samples'\n",
    "            ' in `dl`'\n",
    "            ' len(dl): {}, len(du) {}'.\n",
    "            format(len(uncert_samp_idx), len(dl.sampler.indices),\n",
    "                   len(du.sampler.indices)))\n",
    "\n",
    "        # get high confidence samples `dh`\n",
    "        hcs_idx, hcs_labels = get_high_confidence_samples(pred_prob=pred_prob,\n",
    "                                                          delta=delta_0)\n",
    "        # get the original indices\n",
    "        hcs_idx = [du.sampler.indices[idx] for idx in hcs_idx]\n",
    "\n",
    "        # remove the samples that already selected as uncertain samples.\n",
    "        hcs_idx = [x for x in hcs_idx if\n",
    "                   x not in list(set(uncert_samp_idx) & set(hcs_idx))]\n",
    "\n",
    "        # add high confidence samples to the labeled set 'dl'\n",
    "\n",
    "        # (1) update the indices\n",
    "        dl.sampler.indices.extend(hcs_idx)\n",
    "        # (2) update the original labels with the pseudo labels.\n",
    "        for idx in range(len(hcs_idx)):\n",
    "            dl.dataset.labels[hcs_idx[idx]] = hcs_labels[idx]\n",
    "        logger.info(\n",
    "            'Update size of `dl`  and `du` by adding {} hcs samples in `dl`'\n",
    "            ' len(dl): {}, len(du) {}'.\n",
    "            format(len(hcs_idx), len(dl.sampler.indices),\n",
    "                   len(du.sampler.indices)))\n",
    "\n",
    "        if iteration % t == 0:\n",
    "            logger.info('Iteration: {} fine-tune the model on dh U dl'.\n",
    "                        format(iteration))\n",
    "            model.train(epochs=epochs, train_loader=dl)\n",
    "\n",
    "            # update delta_0\n",
    "            delta_0 = update_threshold(delta=delta_0, dr=dr, t=iteration)\n",
    "\n",
    "        # remove the uncertain samples from the original `du`\n",
    "        logger.info('remove {} uncertain samples from du'.\n",
    "                    format(len(uncert_samp_idx)))\n",
    "        for val in uncert_samp_idx:\n",
    "            du.sampler.indices.remove(val)\n",
    "\n",
    "        p, r, f = model.evaluate(test_loader=dtest)\n",
    "        print(\n",
    "            \"Iteration: {}, len(dl): {}, len(du): {},\"\n",
    "            \" len(dh) {}, p: {} r: {} f: {} \".format(\n",
    "                iteration, len(dl.sampler.indices),\n",
    "                len(du.sampler.indices), len(hcs_idx), p, r, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-7ef34df85d67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m          \u001b[0mRandomCrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m          \u001b[0mNormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m          ToTensor()]))\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m dataset_test = GameImageDataset(\n",
      "\u001b[0;32m<ipython-input-4-90b22381eafc>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root_dir, transform)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# load data and labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/train'"
     ]
    }
   ],
   "source": [
    "dataset_train = GameImageDataset(\n",
    "    root_dir=\"data/train\",\n",
    "    transform=transforms.Compose(\n",
    "        [SquarifyImage(),\n",
    "         RandomCrop(224),\n",
    "         Normalize(),\n",
    "         ToTensor()]))\n",
    "\n",
    "dataset_test = GameImageDataset(\n",
    "    root_dir=\"data/test\",\n",
    "    transform=transforms.Compose(\n",
    "        [SquarifyImage(),\n",
    "         RandomCrop(224),\n",
    "         Normalize(),\n",
    "         ToTensor()]))\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "random_seed = 123\n",
    "validation_split = 0.1  # 10%\n",
    "shuffling_dataset = True\n",
    "batch_size = 16\n",
    "dataset_size = len(dataset_train)\n",
    "\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "\n",
    "if shuffling_dataset:\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "du = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size,\n",
    "                                 sampler=train_sampler, num_workers=4)\n",
    "dl = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size,\n",
    "                                 sampler=valid_sampler, num_workers=4)\n",
    "dtest = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size,\n",
    "                                    num_workers=4)\n",
    "\n",
    "ceal_learning_algorithm(du=du, dl=dl, dtest=dtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra processing\n",
    "---\n",
    "The cells are converted to markdown cells so that they are not run when you run all cells. Change them to code cells to run if required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exporting data zip file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('annotated-20210501T142205Z-001.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move files to train and test directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import os\n",
    "dataitems = os.listdir('data')\n",
    "finalitems = []\n",
    "for item in dataitems:\n",
    "    if not os.path.isdir('data/'+item):\n",
    "        finalitems.append(item)\n",
    "\n",
    "for idx, item in enumerate(finalitems):\n",
    "    # this will assign 20% images(every 5th image) to the test directory\n",
    "    if idx % 5 == 0: \n",
    "        os.rename('data/' + item, 'data/test/'+item)\n",
    "    else:\n",
    "        os.rename('data/' + item, 'data/train/'+item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
